{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAARKlfKAEhw"
      },
      "source": [
        "**Installing Required Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FdtDddM850y",
        "outputId": "7ca642ee-173b-456e-e0c5-1fc5383de983",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==1.13.1\n",
            "  Downloading torch-1.13.1-cp310-cp310-manylinux1_x86_64.whl.metadata (24 kB)\n",
            "Collecting torchvision==0.14.1\n",
            "  Downloading torchvision-0.14.1-cp310-cp310-manylinux1_x86_64.whl.metadata (11 kB)\n",
            "Collecting torchaudio==0.13.1\n",
            "  Downloading torchaudio-0.13.1-cp310-cp310-manylinux1_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1) (4.12.2)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==1.13.1)\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch==1.13.1)\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch==1.13.1)\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==1.13.1)\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.14.1) (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.14.1) (2.32.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.14.1) (11.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1) (75.1.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1) (0.45.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.1) (2024.8.30)\n",
            "Downloading torch-1.13.1-cp310-cp310-manylinux1_x86_64.whl (887.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.5/887.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.14.1-cp310-cp310-manylinux1_x86_64.whl (24.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.2/24.2 MB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchaudio-0.13.1-cp310-cp310-manylinux1_x86_64.whl (4.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m69.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m95.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, nvidia-cudnn-cu11, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.1+cu121\n",
            "    Uninstalling torch-2.5.1+cu121:\n",
            "      Successfully uninstalled torch-2.5.1+cu121\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.20.1+cu121\n",
            "    Uninstalling torchvision-0.20.1+cu121:\n",
            "      Successfully uninstalled torchvision-0.20.1+cu121\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.5.1+cu121\n",
            "    Uninstalling torchaudio-2.5.1+cu121:\n",
            "      Successfully uninstalled torchaudio-2.5.1+cu121\n",
            "Successfully installed nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 torch-1.13.1 torchaudio-0.13.1 torchvision-0.14.1\n",
            "Collecting transformers==4.41.0\n",
            "  Downloading transformers-4.41.0-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.41.0) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.41.0) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.41.0) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.41.0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.41.0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.41.0) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.41.0) (2.32.3)\n",
            "Collecting tokenizers<0.20,>=0.19 (from transformers==4.41.0)\n",
            "  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.41.0) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.41.0) (4.66.6)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.41.0) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.41.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.41.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.41.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.41.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.41.0) (2024.8.30)\n",
            "Downloading transformers-4.41.0-py3-none-any.whl (9.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m78.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.20.3\n",
            "    Uninstalling tokenizers-0.20.3:\n",
            "      Successfully uninstalled tokenizers-0.20.3\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.46.2\n",
            "    Uninstalling transformers-4.46.2:\n",
            "      Successfully uninstalled transformers-4.46.2\n",
            "Successfully installed tokenizers-0.19.1 transformers-4.41.0\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch==1.13.1 torchvision==0.14.1 torchaudio==0.13.1\n",
        "!pip install transformers==4.41.0\n",
        "import torch\n",
        "!pip install numpy pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0i65ORxlYDgF"
      },
      "source": [
        "**Data Loading and Preparation**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PiSNNszZHF_g",
        "outputId": "bb76c916-e9b6-4280-946c-7fc58f627fe5",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing /content/drive/MyDrive/Big Data/LLMs in Education/Autho Scoring Essay-NPCR Model/data-set/asap/fold_1/test.tsv with IDs from /content/drive/MyDrive/Big Data/LLMs in Education/Autho Scoring Essay-NPCR Model/data-set/asap/fold_1/test_ids.txt\n",
            "Processing /content/drive/MyDrive/Big Data/LLMs in Education/Autho Scoring Essay-NPCR Model/data-set/asap/fold_1/train.tsv with IDs from /content/drive/MyDrive/Big Data/LLMs in Education/Autho Scoring Essay-NPCR Model/data-set/asap/fold_1/train_ids.txt\n",
            "Processing /content/drive/MyDrive/Big Data/LLMs in Education/Autho Scoring Essay-NPCR Model/data-set/asap/fold_1/dev.tsv with IDs from /content/drive/MyDrive/Big Data/LLMs in Education/Autho Scoring Essay-NPCR Model/data-set/asap/fold_1/dev_ids.txt\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Base path for folder\n",
        "base_path = \"/content/drive/MyDrive/Big Data/LLMs in Education/Autho Scoring Essay-NPCR Model/data-set/asap\"\n",
        "preprocess_script = \"/content/drive/MyDrive/Big Data/LLMs in Education/Autho Scoring Essay-NPCR Model/data-set/asap/preprocess_asap.py\"\n",
        "\n",
        "# Path for a single fold (fold_1)\n",
        "fold_path = os.path.join(base_path, \"fold_1\")\n",
        "\n",
        "# Input files for the single fold\n",
        "for data_type in ['test', 'train', 'dev']:\n",
        "    data_file = os.path.join(fold_path, f\"{data_type}.tsv\")\n",
        "    ids_file = os.path.join(fold_path, f\"{data_type}_ids.txt\")\n",
        "\n",
        "    print(f\"Processing {data_file} with IDs from {ids_file}\")\n",
        "    os.system(f'python \"{preprocess_script}\" -i \"{data_file}\" -id \"{ids_file}\"')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgmgqUoZo5XU"
      },
      "source": [
        "**Main Running**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_ipZVxrd42S",
        "outputId": "5baba5af-9455-47c0-e145-ca409f796137",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5ZyOipseDoW"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.data.path.append('/root/nltk_data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIzQ5siGeG6J",
        "outputId": "2a2a58e6-e775-4b5e-9d1f-1416d08fb92c",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', 'this', 'is', 'a', 'test', 'sentence', 'for', 'tokenization', '.']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "text = \"Hello, this is a test sentence for tokenization.\"\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3oyLd_mtpA92",
        "outputId": "4aa7efdf-c32d-4037-80eb-dfd2dba15983",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data... - INFO - Prompt id is 2\n",
            "Loading data... - INFO - Reading dataset from: /content/drive/MyDrive/Big Data/LLMs in Education/Autho Scoring Essay-NPCR Model/data-set/asap/fold_1/train.tsv\n",
            "Loading data... - INFO - Reading dataset from: /content/drive/MyDrive/Big Data/LLMs in Education/Autho Scoring Essay-NPCR Model/data-set/asap/fold_1/dev.tsv\n",
            "Loading data... - INFO - Reading dataset from: /content/drive/MyDrive/Big Data/LLMs in Education/Autho Scoring Essay-NPCR Model/data-set/asap/fold_1/test.tsv\n",
            "Prepare data ... - INFO - Statistics:\n",
            "Prepare data ... - INFO -   train X shape: (1080, 512)\n",
            "Prepare data ... - INFO -   dev X shape:   (360, 512)\n",
            "Prepare data ... - INFO -   test X shape:  (360, 512)\n",
            "Prepare data ... - INFO -   train Y shape: (1080, 1)\n",
            "Prepare data ... - INFO -   dev Y shape:   (360, 1)\n",
            "Prepare data ... - INFO -   test Y shape:  (360, 1)\n",
            "/content/drive/MyDrive/Big Data/LLMs in Education/Autho Scoring Essay-NPCR Model/aes-neural-pairwise-contrastive-regression/data_prepare.py:101: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
            "  train_x0 = torch.LongTensor(train_x0)\n",
            "Train... - INFO - ----------------------------------------------------\n",
            "Train... - INFO - Train model\n",
            "Train... - INFO - Epoch 0/20\n",
            "epoch: 0 step: 0 loss: 0.032448407262563705\n",
            "epoch: 0 step: 1 loss: 0.018552467226982117\n",
            "epoch: 0 step: 2 loss: 0.013947615399956703\n",
            "epoch: 0 step: 3 loss: 0.010845717042684555\n",
            "epoch: 0 step: 4 loss: 0.02081519365310669\n",
            "epoch: 0 step: 5 loss: 0.01946011744439602\n",
            "epoch: 0 step: 6 loss: 0.00784901063889265\n",
            "epoch: 0 step: 7 loss: 0.01109339576214552\n",
            "epoch: 0 step: 8 loss: 0.019842153415083885\n",
            "epoch: 0 step: 9 loss: 0.012799233198165894\n",
            "epoch: 0 step: 10 loss: 0.009635401889681816\n",
            "epoch: 0 step: 11 loss: 0.0062217069789767265\n",
            "epoch: 0 step: 12 loss: 0.016357192769646645\n",
            "epoch: 0 step: 13 loss: 0.008952722884714603\n",
            "epoch: 0 step: 14 loss: 0.006301902234554291\n",
            "epoch: 0 step: 15 loss: 0.014798152260482311\n",
            "epoch: 0 step: 16 loss: 0.019088024273514748\n",
            "epoch: 0 step: 17 loss: 0.0078238844871521\n",
            "epoch: 0 step: 18 loss: 0.02160542830824852\n",
            "epoch: 0 step: 19 loss: 0.031000301241874695\n",
            "epoch: 0 step: 20 loss: 0.01088243443518877\n",
            "epoch: 0 step: 21 loss: 0.0074967327527701855\n",
            "epoch: 0 step: 22 loss: 0.007653787266463041\n",
            "epoch: 0 step: 23 loss: 0.010415083728730679\n",
            "epoch: 0 step: 24 loss: 0.014875856228172779\n",
            "epoch: 0 step: 25 loss: 0.005851678550243378\n",
            "epoch: 0 step: 26 loss: 0.002392815425992012\n",
            "epoch: 0 step: 27 loss: 0.01081144530326128\n",
            "epoch: 0 step: 28 loss: 0.008106592111289501\n",
            "epoch: 0 step: 29 loss: 0.0110840555280447\n",
            "epoch: 0 step: 30 loss: 0.019408291205763817\n",
            "epoch: 0 step: 31 loss: 0.010851792059838772\n",
            "epoch: 0 step: 32 loss: 0.014146680012345314\n",
            "epoch: 0 step: 33 loss: 0.0049982257187366486\n",
            "epoch: 0 step: 34 loss: 0.00967735517770052\n",
            "epoch: 0 step: 35 loss: 0.00940158125013113\n",
            "epoch: 0 step: 36 loss: 0.009335227310657501\n",
            "epoch: 0 step: 37 loss: 0.020912496373057365\n",
            "epoch: 0 step: 38 loss: 0.01375215407460928\n",
            "epoch: 0 step: 39 loss: 0.02233049087226391\n",
            "epoch: 0 step: 40 loss: 0.00957759190350771\n",
            "epoch: 0 step: 41 loss: 0.016255071386694908\n",
            "epoch: 0 step: 42 loss: 0.00930516142398119\n",
            "epoch: 0 step: 43 loss: 0.010323881171643734\n",
            "epoch: 0 step: 44 loss: 0.0042144362814724445\n",
            "epoch: 0 step: 45 loss: 0.010938047431409359\n",
            "epoch: 0 step: 46 loss: 0.010469160974025726\n",
            "epoch: 0 step: 47 loss: 0.003491715295240283\n",
            "epoch: 0 step: 48 loss: 0.011504463851451874\n",
            "epoch: 0 step: 49 loss: 0.010898011736571789\n",
            "epoch: 0 step: 50 loss: 0.010441269725561142\n",
            "epoch: 0 step: 51 loss: 0.00610151793807745\n",
            "epoch: 0 step: 52 loss: 0.007332672365009785\n",
            "epoch: 0 step: 53 loss: 0.005476852413266897\n",
            "epoch: 0 step: 54 loss: 0.005522413644939661\n",
            "epoch: 0 step: 55 loss: 0.008508111350238323\n",
            "epoch: 0 step: 56 loss: 0.009040184319019318\n",
            "epoch: 0 step: 57 loss: 0.00635853735730052\n",
            "epoch: 0 step: 58 loss: 0.007057117763906717\n",
            "epoch: 0 step: 59 loss: 0.005376273300498724\n",
            "epoch: 0 step: 60 loss: 0.008591864258050919\n",
            "epoch: 0 step: 61 loss: 0.006770648993551731\n",
            "epoch: 0 step: 62 loss: 0.00879698432981968\n",
            "epoch: 0 step: 63 loss: 0.0077353003434836864\n",
            "epoch: 0 step: 64 loss: 0.007222309708595276\n",
            "epoch: 0 step: 65 loss: 0.009060108102858067\n",
            "epoch: 0 step: 66 loss: 0.003460518317297101\n",
            "epoch: 0 step: 67 loss: 0.011761373840272427\n",
            "epoch: 0 step: 68 loss: 0.008048116229474545\n",
            "epoch: 0 step: 69 loss: 0.006784379016608\n",
            "epoch: 0 step: 70 loss: 0.004937077406793833\n",
            "epoch: 0 step: 71 loss: 0.002111191861331463\n",
            "epoch: 0 step: 72 loss: 0.005239019636064768\n",
            "epoch: 0 step: 73 loss: 0.014129163697361946\n",
            "epoch: 0 step: 74 loss: 0.004768272861838341\n",
            "epoch: 0 step: 75 loss: 0.009497075341641903\n",
            "epoch: 0 step: 76 loss: 0.00614065770059824\n",
            "epoch: 0 step: 77 loss: 0.005912106484174728\n",
            "epoch: 0 step: 78 loss: 0.014495438896119595\n",
            "epoch: 0 step: 79 loss: 0.007088008802384138\n",
            "epoch: 0 step: 80 loss: 0.005315359681844711\n",
            "epoch: 0 step: 81 loss: 0.003052884479984641\n",
            "epoch: 0 step: 82 loss: 0.0063254195265471935\n",
            "epoch: 0 step: 83 loss: 0.005353141576051712\n",
            "epoch: 0 step: 84 loss: 0.008883565664291382\n",
            "epoch: 0 step: 85 loss: 0.009623469784855843\n",
            "epoch: 0 step: 86 loss: 0.009774752892553806\n",
            "epoch: 0 step: 87 loss: 0.008188224397599697\n",
            "epoch: 0 step: 88 loss: 0.010278083384037018\n",
            "epoch: 0 step: 89 loss: 0.005902122240513563\n",
            "epoch: 0 step: 90 loss: 0.006343322340399027\n",
            "epoch: 0 step: 91 loss: 0.0065965489484369755\n",
            "epoch: 0 step: 92 loss: 0.00802075956016779\n",
            "epoch: 0 step: 93 loss: 0.00532416719943285\n",
            "epoch: 0 step: 94 loss: 0.005296958144754171\n",
            "epoch: 0 step: 95 loss: 0.0054607572965323925\n",
            "epoch: 0 step: 96 loss: 0.00927791465073824\n",
            "epoch: 0 step: 97 loss: 0.007981212809681892\n",
            "epoch: 0 step: 98 loss: 0.0028891090769320726\n",
            "epoch: 0 step: 99 loss: 0.007078902330249548\n",
            "epoch: 0 step: 100 loss: 0.010021494701504707\n",
            "epoch: 0 step: 101 loss: 0.003623098600655794\n",
            "epoch: 0 step: 102 loss: 0.00954520609229803\n",
            "epoch: 0 step: 103 loss: 0.00785891991108656\n",
            "epoch: 0 step: 104 loss: 0.0030877951066941023\n",
            "epoch: 0 step: 105 loss: 0.008200694806873798\n",
            "epoch: 0 step: 106 loss: 0.013804355636239052\n",
            "epoch: 0 step: 107 loss: 0.011717616580426693\n",
            "epoch: 0 step: 108 loss: 0.004190447274595499\n",
            "epoch: 0 step: 109 loss: 0.011743105947971344\n",
            "epoch: 0 step: 110 loss: 0.00439820671454072\n",
            "epoch: 0 step: 111 loss: 0.0032028169371187687\n",
            "epoch: 0 step: 112 loss: 0.005739722400903702\n",
            "epoch: 0 step: 113 loss: 0.004191109444946051\n",
            "epoch: 0 step: 114 loss: 0.018284602090716362\n",
            "epoch: 0 step: 115 loss: 0.015016679652035236\n",
            "epoch: 0 step: 116 loss: 0.009560205042362213\n",
            "epoch: 0 step: 117 loss: 0.00579385831952095\n",
            "epoch: 0 step: 118 loss: 0.009830250404775143\n",
            "epoch: 0 step: 119 loss: 0.00915628019720316\n",
            "epoch: 0 step: 120 loss: 0.0023870468139648438\n",
            "epoch: 0 step: 121 loss: 0.009790566749870777\n",
            "epoch: 0 step: 122 loss: 0.004967856220901012\n",
            "epoch: 0 step: 123 loss: 0.007318283896893263\n",
            "epoch: 0 step: 124 loss: 0.008236268535256386\n",
            "epoch: 0 step: 125 loss: 0.010030783712863922\n",
            "epoch: 0 step: 126 loss: 0.0031211941968649626\n",
            "epoch: 0 step: 127 loss: 0.005427312571555376\n",
            "epoch: 0 step: 128 loss: 0.006926220841705799\n",
            "epoch: 0 step: 129 loss: 0.013651859946548939\n",
            "epoch: 0 step: 130 loss: 0.007520909886807203\n",
            "epoch: 0 step: 131 loss: 0.010180080309510231\n",
            "epoch: 0 step: 132 loss: 0.008865000680088997\n",
            "epoch: 0 step: 133 loss: 0.006519193295389414\n",
            "epoch: 0 step: 134 loss: 0.006623709108680487\n",
            "epoch: 0 step: 135 loss: 0.004439652897417545\n",
            "epoch: 0 step: 136 loss: 0.006721761077642441\n",
            "epoch: 0 step: 137 loss: 0.004907610826194286\n",
            "epoch: 0 step: 138 loss: 0.004882762674242258\n",
            "epoch: 0 step: 139 loss: 0.009765070863068104\n",
            "epoch: 0 step: 140 loss: 0.007203295361250639\n",
            "epoch: 0 step: 141 loss: 0.00822709035128355\n",
            "epoch: 0 step: 142 loss: 0.006649959832429886\n",
            "epoch: 0 step: 143 loss: 0.006670920643955469\n",
            "epoch: 0 step: 144 loss: 0.006902893539518118\n",
            "epoch: 0 step: 145 loss: 0.008057876490056515\n",
            "epoch: 0 step: 146 loss: 0.005912366788834333\n",
            "epoch: 0 step: 147 loss: 0.005699797999113798\n",
            "epoch: 0 step: 148 loss: 0.008246795274317265\n",
            "epoch: 0 step: 149 loss: 0.0037727223243564367\n",
            "epoch: 0 step: 150 loss: 0.00339648500084877\n",
            "epoch: 0 step: 151 loss: 0.010173958726227283\n",
            "epoch: 0 step: 152 loss: 0.0027351644821465015\n",
            "epoch: 0 step: 153 loss: 0.0031072061974555254\n",
            "epoch: 0 step: 154 loss: 0.0040773386135697365\n",
            "epoch: 0 step: 155 loss: 0.004383044317364693\n",
            "epoch: 0 step: 156 loss: 0.009230280295014381\n",
            "epoch: 0 step: 157 loss: 0.009751836769282818\n",
            "epoch: 0 step: 158 loss: 0.0057968623004853725\n",
            "epoch: 0 step: 159 loss: 0.008232856169342995\n",
            "epoch: 0 step: 160 loss: 0.014439389109611511\n",
            "epoch: 0 step: 161 loss: 0.0024350564926862717\n",
            "epoch: 0 step: 162 loss: 0.004225538112223148\n",
            "epoch: 0 step: 163 loss: 0.005895758047699928\n",
            "epoch: 0 step: 164 loss: 0.0047263591550290585\n",
            "epoch: 0 step: 165 loss: 0.00794108584523201\n",
            "epoch: 0 step: 166 loss: 0.006054689642041922\n",
            "epoch: 0 step: 167 loss: 0.007435105741024017\n",
            "epoch: 0 step: 168 loss: 0.008958171121776104\n",
            "epoch: 0 step: 169 loss: 0.005809243302792311\n",
            "epoch: 0 step: 170 loss: 0.00748889846727252\n",
            "epoch: 0 step: 171 loss: 0.0027028874028474092\n",
            "epoch: 0 step: 172 loss: 0.0016626546857878566\n",
            "epoch: 0 step: 173 loss: 0.0025250050239264965\n",
            "epoch: 0 step: 174 loss: 0.0033519582357257605\n",
            "epoch: 0 step: 175 loss: 0.0025892804842442274\n",
            "epoch: 0 step: 176 loss: 0.005690725985914469\n",
            "epoch: 0 step: 177 loss: 0.004302512388676405\n",
            "epoch: 0 step: 178 loss: 0.0020646597258746624\n",
            "epoch: 0 step: 179 loss: 0.004817103501409292\n",
            "epoch: 0 step: 180 loss: 0.00801805593073368\n",
            "epoch: 0 step: 181 loss: 0.009598354808986187\n",
            "epoch: 0 step: 182 loss: 0.006089028436690569\n",
            "epoch: 0 step: 183 loss: 0.0056679933331906796\n",
            "epoch: 0 step: 184 loss: 0.005238489713519812\n",
            "epoch: 0 step: 185 loss: 0.005093215964734554\n",
            "epoch: 0 step: 186 loss: 0.005217483267188072\n",
            "epoch: 0 step: 187 loss: 0.003256289754062891\n",
            "epoch: 0 step: 188 loss: 0.001576726557686925\n",
            "epoch: 0 step: 189 loss: 0.0025368810165673494\n",
            "epoch: 0 step: 190 loss: 0.003106215037405491\n",
            "epoch: 0 step: 191 loss: 0.007775855716317892\n",
            "epoch: 0 step: 192 loss: 0.002668377012014389\n",
            "epoch: 0 step: 193 loss: 0.004662968683987856\n",
            "epoch: 0 step: 194 loss: 0.007578322198241949\n",
            "epoch: 0 step: 195 loss: 0.003829085500910878\n",
            "epoch: 0 step: 196 loss: 0.007299903314560652\n",
            "epoch: 0 step: 197 loss: 0.004004358779639006\n",
            "epoch: 0 step: 198 loss: 0.007114398758858442\n",
            "epoch: 0 step: 199 loss: 0.003514899406582117\n",
            "epoch: 0 step: 200 loss: 0.005564925726503134\n",
            "epoch: 0 step: 201 loss: 0.0005510752089321613\n",
            "Train... - INFO - Training one epoch in 403.295 s\n"
          ]
        }
      ],
      "source": [
        "!python \"/content/drive/MyDrive/Big Data/LLMs in Education/Autho Scoring Essay-NPCR Model/aes-neural-pairwise-contrastive-regression/main.py\" --oov random --checkpoint_path \"/content/drive/MyDrive/Big Data/LLMs in Education/Autho Scoring Essay-NPCR Model/checkpoint\" --train \"/content/drive/MyDrive/Big Data/LLMs in Education/Autho Scoring Essay-NPCR Model/data-set/asap/fold_1/train.tsv\" --dev \"/content/drive/MyDrive/Big Data/LLMs in Education/Autho Scoring Essay-NPCR Model/data-set/asap/fold_1/dev.tsv\" --test \"/content/drive/MyDrive/Big Data/LLMs in Education/Autho Scoring Essay-NPCR Model/data-set/asap/fold_1/test.tsv\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}